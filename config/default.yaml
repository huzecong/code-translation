random_seed: 1234

data:
  training_set: "data/processed_notok/train.txt"
  valid_sets:
    repos_included: "data/processed_notok/valid.txt"
    repos_excluded: "data/processed_notok/valid_exclude.txt"
  test_sets:
    repos_included: "data/processed_notok/test.txt"
    repos_excluded: "data/processed_notok/test_exclude.txt"
  vocab_file: "data/vocab.vocab"
  verbose: true
  hparams:
    tuple_delimiter: "\u0001"
    token_delimiter: "\u0000"
    max_src_len: 512
    max_tgt_len: 512
    spm_model: "data/vocab.model"

model:
  hidden_dim: 512
  max_sentence_length: 512
  loss_label_confidence: 0.9

lr_scheduler:
  schedule: warmup
  init_lr: 0.0883  # 2 / sqrt(hidden_dim)
  warmup_steps: 16000

training:
  max_batch_tokens: 4096
  test_batch_size: 16

  max_train_steps: 1000000
  display_steps: 500
  eval_steps: 10000

inference:
  beam_width: 5
  length_penalty: 0.6

lr_scheduler:
  schedule: warmup
  init_lr: 0.0001
  warmup_steps: 8000

training:
  max_batch_tokens: 2048
  max_train_steps: 20000
  eval_steps: 20000
